{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a608f90b",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ee71d3",
   "metadata": {},
   "source": [
    "This file is the data preprocessing part of the project, submitted to Prof. Soong Moon Kang for MSIN0074 Network Analysis by SRN 22086573."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32510e35",
   "metadata": {},
   "source": [
    "### Importing Libraries and Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa853a0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/ijeonghyeon/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ijeonghyeon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ijeonghyeon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ijeonghyeon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "import contractions\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from langdetect import detect\n",
    "from collections import Counter\n",
    "import langdetect\n",
    "\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "870ad7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"twitter_concat.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c39158c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 79105 entries, 0 to 79104\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   id                   79099 non-null  object \n",
      " 1   text                 79085 non-null  object \n",
      " 2   user_id              79065 non-null  object \n",
      " 3   timestamp            79065 non-null  object \n",
      " 4   retweet_count        79065 non-null  float64\n",
      " 5   favorite_count       79055 non-null  float64\n",
      " 6   in_reply_to_user_id  14908 non-null  object \n",
      " 7   twt_hashtags         79065 non-null  object \n",
      " 8   user_name            79063 non-null  object \n",
      " 9   followers_count      79065 non-null  float64\n",
      " 10  friends_count        79045 non-null  float64\n",
      "dtypes: float64(4), object(7)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b47a6",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bdcff09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080\n",
      "id                         6\n",
      "text                      20\n",
      "user_id                   40\n",
      "timestamp                 40\n",
      "retweet_count             40\n",
      "favorite_count            50\n",
      "in_reply_to_user_id    64197\n",
      "twt_hashtags              40\n",
      "user_name                 42\n",
      "followers_count           40\n",
      "friends_count             60\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.duplicated().sum())\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3af25058",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n",
    "df = df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb8b7392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 78013 entries, 0 to 79104\n",
      "Data columns (total 11 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   id                   78013 non-null  object \n",
      " 1   text                 78013 non-null  object \n",
      " 2   user_id              78005 non-null  object \n",
      " 3   timestamp            78005 non-null  object \n",
      " 4   retweet_count        78005 non-null  float64\n",
      " 5   favorite_count       77996 non-null  float64\n",
      " 6   in_reply_to_user_id  14789 non-null  object \n",
      " 7   twt_hashtags         78005 non-null  object \n",
      " 8   user_name            78003 non-null  object \n",
      " 9   followers_count      78005 non-null  float64\n",
      " 10  friends_count        77986 non-null  float64\n",
      "dtypes: float64(4), object(7)\n",
      "memory usage: 7.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43907817",
   "metadata": {},
   "source": [
    "### Text Preprocessing - Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cfd56fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of negation cues\n",
    "negation_cues = [\"not\", \"n't\", \"never\", \"no\", \"none\", \"neither\", \"nor\"]\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    # Convert to lowercase\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # Tokenize the text\n",
    "    text_tokens = nltk.word_tokenize(text)\n",
    "    # Handle negation cues\n",
    "    negated = False\n",
    "    for i, token in enumerate(text_tokens):\n",
    "        if token.lower() in negation_cues:\n",
    "            negated = True\n",
    "        elif negated:\n",
    "            text_tokens[i] = \"NOT_\" + token\n",
    "            negated = False\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text = [word for word in text_tokens if not word in stop_words]\n",
    "    # Join the filtered words back into a string\n",
    "    text = ' '.join(filtered_text)\n",
    "    # Replace contractions with their expanded form\n",
    "    text = contractions.fix(text)\n",
    "    return text\n",
    "\n",
    "tweets = df.text\n",
    "processed_tweets = []\n",
    "for text in tweets:\n",
    "    result = text_preprocessing(text)\n",
    "    processed_tweets.append(result)\n",
    "    \n",
    "processed_tweets = pd.Series(processed_tweets)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Define a function that takes a sentence as input and returns a list of lemmas\n",
    "def lemmatize_nltk(sentence):\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    # Perform part-of-speech tagging on the tokens \n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmas = []\n",
    "    for token, tag in pos_tags:\n",
    "        # Map the POS tag to the corresponding WordNet POS tag\n",
    "        tag = get_wordnet_pos(tag)\n",
    "        if tag:\n",
    "            lemma = lemmatizer.lemmatize(token, tag)\n",
    "        else:\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "# Define a function that maps NLTK POS tags to WordNet POS tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "lemmatized_words = []\n",
    "for sentence in processed_tweets:\n",
    "    lemmas = lemmatize_nltk(sentence)\n",
    "    lemmatized_words.append(lemmas)\n",
    "    \n",
    "# Convert the list of lemmatized words to a Series\n",
    "lemmatised_tweets = pd.Series(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29422d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do', 'not', 'like', 'long', 'boring', 'movie']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with the text_preprocessing model\n",
    "text = \"I don't like the long and boring movie.\"\n",
    "text = text_preprocessing(text)\n",
    "text = lemmatize_nltk(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c3fb39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to remove\n",
    "remove_words_tw = [\"rt\",\"be\",\"de\",\"get\",\"do\",\"use\",\"one\",\"la\",\"en\",\"‚Äô\",\"u\"] \n",
    "\n",
    "# Remove words from each list using list comprehension\n",
    "tweets = [[word for word in lst if word not in remove_words_tw] for lst in lemmatised_tweets]\n",
    "\n",
    "# Remove the numbers from each list and clean hashtags for final\n",
    "preprocessed_tweets = [[word for word in tweet if not str(word).isnumeric()] for tweet in tweets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11813784",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tweet_corpus = []\n",
    "\n",
    "for doc in preprocessed_tweets:\n",
    "    sentence = \" \".join(doc)\n",
    "    pre_tweet_corpus.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6e8fabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting the language and extract only english sentences\n",
    "tweet_corpus = []\n",
    "\n",
    "for tweet in pre_tweet_corpus:\n",
    "    try:\n",
    "        lang = detect(tweet)\n",
    "        if lang == \"en\":\n",
    "            tweet_corpus.append(tweet)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cf1fb8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['üî• denet giveaway ÔºÅüî• üèÜ reward poolsÔºö823925 worth fb token ‚úÖ follow ‚úÖ like amp ‚úÖ complete denet task ‚§µÔ∏è httpstco3m84n8brg6 üîîtip invite likely cult pinetwork rio airdrop usdc busd bitcoin giveaway denet',\n",
       " 'bitcoin pump original narrative absolutely love see',\n",
       " 'adam3us great article cover multiple reason bitcoin adoption network effect usual technology',\n",
       " 'airdropinspect new airdrop kollect usdt total reward usdt rate ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è winner random amp top distribution within we‚Ä¶',\n",
       " 'getyafacemelted btc amp crypto look strong af would NOT_surprise test 30k minimum next day bank distress ‚Ä¶']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "baed883d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59151"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d78baf",
   "metadata": {},
   "source": [
    "### Text preprocessing - Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "feb0321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = df.twt_hashtags\n",
    "\n",
    "processed_hashtags = []\n",
    "for text in hashtags:\n",
    "    result = text_preprocessing(text)\n",
    "    processed_hashtags.append(result)\n",
    "    \n",
    "processed_hashtags = pd.Series(processed_hashtags)\n",
    "\n",
    "lemmatized_hashtags = []\n",
    "for sentence in processed_hashtags:\n",
    "    lemmas = lemmatize_nltk(sentence)\n",
    "    lemmatized_hashtags.append(lemmas)\n",
    "    \n",
    "lemmatised_hashtags = pd.Series(lemmatized_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "013957c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to remove\n",
    "remove_words = ['text', 'indices','index']\n",
    "\n",
    "# Remove words from each list using list comprehension\n",
    "hashtags = [[word for word in lst if word not in remove_words] for lst in lemmatised_hashtags]\n",
    "\n",
    "# Remove the numbers from each list and clean hashtags for final\n",
    "preprocessed_hashtags = [[word for word in hashtag if not str(word).isnumeric()] for hashtag in hashtags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48b6573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_hashtag_corpus = []\n",
    "\n",
    "for doc in preprocessed_hashtags:\n",
    "    sentence = \" \".join(doc)\n",
    "    pre_hashtag_corpus.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8ae12ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting the language and extract only english sentences\n",
    "\n",
    "hashtag_corpus = []\n",
    "\n",
    "for sentence in pre_hashtag_corpus:\n",
    "    try:\n",
    "        lang = langdetect.detect(sentence)\n",
    "        if lang == \"en\":\n",
    "            hashtag_corpus.append(sentence)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac4d47f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pinetwork airdrop usdc busd bitcoin giveaway denet',\n",
       " 'btc',\n",
       " 'bitcoin',\n",
       " 'bitcoin giveaway',\n",
       " 'bitcoin']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b087e901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'tweet_corpus' (list)\n",
      "Stored 'hashtag_corpus' (list)\n",
      "Stored 'df' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store tweet_corpus hashtag_corpus df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
